# =============================================================================
# AI Model Distillation for Financial Data - CI
# =============================================================================
#
# This workflow validates the Data Flywheel deployment using Docker Compose.
#
# IMPORTANT NOTES:
# ================
# 1. NMP (NeMo Microservices Platform) Limitation:
#    The full notebook requires NMP which needs Kubernetes/Minikube.
#    CI runners only have Docker, so NMP-related cells are SKIPPED.
#    Skipped functionality:
#    - Part 1: Teacher Model Label Generation (cells 10-40)
#    - Part 2 NMP calls: Model deployment to nemo.test (cells 19-25, 92, 102)
#
# 2. Test Data Strategy:
#    Since Part 1 (teacher model labeling) is skipped, we load test data
#    directly from Huggingface (ic-fspml/stock_news_sentiment) and transform
#    it to the Data Flywheel format.
#
# 3. What IS validated:
#    - Docker Compose deployment (API, Celery, Redis, MongoDB, Elasticsearch, MLflow)
#    - Data loading to Elasticsearch
#    - API endpoints functionality
#    - Service health checks
#
# Required Secrets:
#   - NGC_API_KEY          : NVIDIA NGC API Key for container registry access
#   - NVIDIA_API_KEY       : NVIDIA API Key for hosted NIM services
#   - MONGO_USERNAME       : MongoDB username (can be empty for default)
#   - MONGO_PASSWORD       : MongoDB password (can be empty for default)
#   - REDIS_PASSWORD       : Redis password (can be empty for default)
#   - HF_TOKEN             : Huggingface token (optional)
#   - SMTP_USERNAME        : Gmail address for email notifications
#   - SMTP_PASSWORD        : Gmail app-specific password
#
# Hardware Requirements:
#   - GPU: 4x NVIDIA GPU (A100/H100 recommended)
#   - Memory: 32GB+ RAM
#   - Storage: 100GB+ free disk space
#
# Service Ports:
#   - API Server:     8000
#   - Elasticsearch:  9200
#   - MongoDB:        27017
#   - Redis:          6379
#   - MLflow:         5000
#
# =============================================================================

name: CI - Data Flywheel

on:
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'deploy/**'
      - 'config/**'
      - 'notebooks/**'
      - 'scripts/**'
      - '.github/workflows/ci.yaml'

  pull_request:
    branches: [main]
    paths:
      - 'src/**'
      - 'deploy/**'
      - 'config/**'
      - 'notebooks/**'
      - 'scripts/**'
      - '.github/workflows/ci.yaml'

  workflow_dispatch:
    # No inputs - runs with default configuration

env:
  # Required secrets
  NVIDIA_API_KEY: ${{ secrets.NVIDIA_API_KEY }}
  NGC_API_KEY: ${{ secrets.NGC_API_KEY }}
  GH_TOKEN: ${{ secrets.GH_TOKEN }}
  MONGO_USERNAME: ${{ secrets.MONGO_USERNAME }}
  MONGO_PASSWORD: ${{ secrets.MONGO_PASSWORD }}
  REDIS_PASSWORD: ${{ secrets.REDIS_PASSWORD }}
  HF_TOKEN: ${{ secrets.HF_TOKEN }}

  # Configuration
  ES_COLLECTION_NAME: flywheel
  TAG: "0.3.0"

  # Test configuration
  TEST_IMAGE: nvcr.io/rw983xdqtcdp/auto_test_team/blueprint-github-test-image:latest
  DFW_API_URL: http://localhost:8000
  ENABLE_EMAIL_NOTIFICATION: true

jobs:
  # ============================================
  # PREFLIGHT: Check all prerequisites
  # ============================================
  preflight:
    name: Preflight Checks
    runs-on: ubuntu-latest
    outputs:
      preflight_passed: ${{ steps.preflight.outputs.passed }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Docker Login to NGC
        env:
          NGC_API_KEY: ${{ secrets.NGC_API_KEY }}
        run: |
          echo "${NGC_API_KEY}" | docker login nvcr.io -u '$oauthtoken' --password-stdin
          echo "✓ NGC Docker login successful"

      - name: Preflight Checks
        id: preflight
        env:
          NGC_API_KEY: ${{ secrets.NGC_API_KEY }}
          NVIDIA_API_KEY: ${{ secrets.NVIDIA_API_KEY }}
        run: |
          echo "========================================"
          echo "  PREFLIGHT CHECKS"
          echo "========================================"
          echo ""
          
          FAILED=0
          
          # Check 1: NGC API Key
          echo "┌──────────────────────────────────────────────┐"
          echo "│ [Check 1/6] NGC API Key                      │"
          echo "└──────────────────────────────────────────────┘"
          if [ -z "${NGC_API_KEY}" ]; then
            echo "✗ ERROR: NGC_API_KEY is not set"
            FAILED=1
          else
            echo "✓ NGC_API_KEY is set"
          fi
          echo ""
          
          # Check 2: NVIDIA API Key
          echo "┌──────────────────────────────────────────────┐"
          echo "│ [Check 2/6] NVIDIA API Key                   │"
          echo "└──────────────────────────────────────────────┘"
          if [ -z "${NVIDIA_API_KEY}" ]; then
            echo "✗ ERROR: NVIDIA_API_KEY is not set"
            FAILED=1
          else
            echo "✓ NVIDIA_API_KEY is set"
          fi
          echo ""
          
          # Check 3: Test Image can be pulled
          echo "┌──────────────────────────────────────────────┐"
          echo "│ [Check 3/6] Test Image                       │"
          echo "└──────────────────────────────────────────────┘"
          echo "Pulling test image: ${{ env.TEST_IMAGE }}"
          if docker pull ${{ env.TEST_IMAGE }}; then
            echo "✓ Test image pulled successfully"
          else
            echo "✗ ERROR: Cannot pull test image"
            FAILED=1
          fi
          echo ""
          
          # Check 4: Required notebook exists
          echo "┌──────────────────────────────────────────────┐"
          echo "│ [Check 4/6] Required Notebook                │"
          echo "└──────────────────────────────────────────────┘"
          if [ -f "notebooks/ai-model-distillation-financial-data.ipynb" ]; then
            echo "✓ notebooks/ai-model-distillation-financial-data.ipynb exists"
          else
            echo "✗ ERROR: Main notebook not found"
            FAILED=1
          fi
          echo ""
          
          # Check 5: Docker Compose files
          echo "┌──────────────────────────────────────────────┐"
          echo "│ [Check 5/6] Docker Compose Files             │"
          echo "└──────────────────────────────────────────────┘"
          if [ -f "deploy/docker-compose.yaml" ]; then
            echo "✓ deploy/docker-compose.yaml exists"
          else
            echo "✗ ERROR: docker-compose.yaml not found"
            FAILED=1
          fi
          if [ -f "scripts/run.sh" ]; then
            echo "✓ scripts/run.sh exists"
          else
            echo "✗ ERROR: scripts/run.sh not found"
            FAILED=1
          fi
          echo ""
          
          # Check 6: Clone qa-tester repository
          echo "┌──────────────────────────────────────────────┐"
          echo "│ [Check 6/6] QA Tester Repository             │"
          echo "└──────────────────────────────────────────────┘"
          echo "Cloning qa-tester repository..."
          if git clone --depth 1 https://x-access-token:${{ github.token }}@github.com/bp-cicd-org/qa-tester.git _qa_tester; then
            if [ -f "_qa_tester/utils/notebook_runner/notebook_runner_nbclient.py" ]; then
              echo "✓ qa-tester cloned and notebook_runner_nbclient.py found"
            else
              echo "✗ ERROR: notebook_runner_nbclient.py not found in qa-tester"
              FAILED=1
            fi
          else
            echo "✗ ERROR: Cannot clone qa-tester repository"
            FAILED=1
          fi
          echo ""
          
          # Final result
          echo "========================================"
          if [ $FAILED -eq 0 ]; then
            echo "✓ ALL PREFLIGHT CHECKS PASSED"
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "✗ PREFLIGHT CHECKS FAILED"
            echo "passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          echo "========================================"

  # ============================================
  # DEPLOY & TEST: Main deployment and validation
  # ============================================
  deploy-and-test:
    name: Deploy & Test Data Flywheel
    needs: preflight
    if: ${{ needs.preflight.outputs.preflight_passed == 'true' }}
    runs-on: arc-runner-set-oke-org-poc-4-gpu
    # runs-on: arc-runners-org-nvidia-ai-bp-4-gpu
    timeout-minutes: 120
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      # ==========================================
      # PHASE 1: Environment Setup
      # ==========================================
      - name: Setup Environment
        run: |
          echo "========================================"
          echo "  AI MODEL DISTILLATION - ENVIRONMENT SETUP"
          echo "========================================"
          
          # System info
          echo "Runner: $(hostname)"
          echo "OS: $(cat /etc/os-release | grep PRETTY_NAME | cut -d'"' -f2)"
          echo "Docker: $(docker --version)"
          echo ""
          
          # GPU info
          echo "GPU Information:"
          nvidia-smi --query-gpu=index,name,memory.total --format=csv,noheader || echo "nvidia-smi not available"
          echo ""

      - name: Docker Login to NGC
        env:
          NGC_API_KEY: ${{ secrets.NGC_API_KEY }}
        run: |
          echo "${NGC_API_KEY}" | docker login nvcr.io -u '$oauthtoken' --password-stdin
          echo "✓ NGC Docker login successful"

      - name: Pull Test Image
        run: |
          echo "Pulling test image: ${{ env.TEST_IMAGE }}"
          docker pull ${{ env.TEST_IMAGE }}
          echo "✓ Test image pulled successfully"

      # ==========================================
      # PHASE 2: Setup Notebook Runner
      # ==========================================
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Download Notebook Runner
        run: |
          echo "Cloning qa-tester repository..."
          git clone --depth 1 https://x-access-token:${{ github.token }}@github.com/bp-cicd-org/qa-tester.git _qa_tester
          cp _qa_tester/utils/notebook_runner/notebook_runner_nbclient.py .
          rm -rf _qa_tester
          chmod +x notebook_runner_nbclient.py
          echo "✓ notebook_runner_nbclient.py ready"

      - name: Install Notebook Dependencies
        run: |
          pip install --upgrade pip
          pip install ipykernel nbclient nbformat jupyter nbconvert papermill
          python -m ipykernel install --user --name python3 --display-name "Python 3"
          echo "✓ Notebook dependencies installed"

      # ==========================================
      # PHASE 3: Pre-configure Environment
      # ==========================================
      - name: Pre-configure Environment Variables
        env:
          NVIDIA_API_KEY: ${{ secrets.NVIDIA_API_KEY }}
          NGC_API_KEY: ${{ secrets.NGC_API_KEY }}
          MONGO_USERNAME: ${{ secrets.MONGO_USERNAME }}
          MONGO_PASSWORD: ${{ secrets.MONGO_PASSWORD }}
          REDIS_PASSWORD: ${{ secrets.REDIS_PASSWORD }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          echo "========================================"
          echo "  PRE-CONFIGURING ENVIRONMENT"
          echo "========================================"
          
          # Debug: Check if secrets are available
          echo "Checking secrets..."
          [ -n "${NVIDIA_API_KEY}" ] && echo "✓ NVIDIA_API_KEY is set" || echo "⚠ NVIDIA_API_KEY not set"
          [ -n "${NGC_API_KEY}" ] && echo "✓ NGC_API_KEY is set" || echo "⚠ NGC_API_KEY not set"
          [ -n "${MONGO_USERNAME}" ] && echo "✓ MONGO_USERNAME is set" || echo "- MONGO_USERNAME not set (using default)"
          [ -n "${MONGO_PASSWORD}" ] && echo "✓ MONGO_PASSWORD is set" || echo "- MONGO_PASSWORD not set (using default)"
          [ -n "${REDIS_PASSWORD}" ] && echo "✓ REDIS_PASSWORD is set" || echo "- REDIS_PASSWORD not set (using default)"
          
          # Set environment variables for deployment
          echo "NVIDIA_API_KEY=${NVIDIA_API_KEY}" >> $GITHUB_ENV
          echo "NGC_API_KEY=${NGC_API_KEY}" >> $GITHUB_ENV
          echo "MONGO_USERNAME=${MONGO_USERNAME:-admin}" >> $GITHUB_ENV
          echo "MONGO_PASSWORD=${MONGO_PASSWORD:-password}" >> $GITHUB_ENV
          echo "REDIS_PASSWORD=${REDIS_PASSWORD:-password}" >> $GITHUB_ENV
          echo "HF_TOKEN=${HF_TOKEN}" >> $GITHUB_ENV
          
          # Create .env file for Docker Compose
          cat > deploy/.env << EOF
          MONGO_USERNAME=${MONGO_USERNAME:-admin}
          MONGO_PASSWORD=${MONGO_PASSWORD:-password}
          REDIS_PASSWORD=${REDIS_PASSWORD:-password}
          NVIDIA_API_KEY=${NVIDIA_API_KEY}
          NGC_API_KEY=${NGC_API_KEY}
          LLM_JUDGE_API_KEY=${NVIDIA_API_KEY}
          EMB_API_KEY=${NVIDIA_API_KEY}
          HF_TOKEN=${HF_TOKEN}
          ES_COLLECTION_NAME=${ES_COLLECTION_NAME}
          TAG=${TAG}
          COMPOSE_PROFILES=mlflow
          EOF
          
          # Also create .env in home directory for notebook
          cat > ~/.env << EOF
          NGC_API_KEY=${NGC_API_KEY}
          NVIDIA_API_KEY=${NVIDIA_API_KEY}
          REDIS_PASSWORD=${REDIS_PASSWORD:-password}
          MONGO_USERNAME=${MONGO_USERNAME:-admin}
          MONGO_PASSWORD=${MONGO_PASSWORD:-password}
          EOF
          
          echo ""
          echo "✓ Environment variables configured"

      # ==========================================
      # PHASE 4: Deploy Services via Docker Compose
      # ==========================================
      # -----------------------------------------------------------------------------
      # DEPLOYMENT STRATEGY
      # -----------------------------------------------------------------------------
      # Instead of running notebook cells for deployment (which have Brev-specific
      # path configurations), we deploy Docker Compose services directly via CI.
      #
      # This is equivalent to running notebook Cell 49 which executes:
      #   ./scripts/run.sh
      #
      # The notebook is executed separately for documentation/artifact purposes,
      # with most cells skipped.
      # -----------------------------------------------------------------------------
      - name: Deploy Services via Docker Compose
        env:
          NGC_API_KEY: ${{ secrets.NGC_API_KEY }}
          NVIDIA_API_KEY: ${{ secrets.NVIDIA_API_KEY }}
          MONGO_USERNAME: ${{ secrets.MONGO_USERNAME }}
          MONGO_PASSWORD: ${{ secrets.MONGO_PASSWORD }}
          REDIS_PASSWORD: ${{ secrets.REDIS_PASSWORD }}
        run: |
          echo "========================================"
          echo "  DEPLOYING DATA FLYWHEEL SERVICES"
          echo "========================================"
          echo ""
          
          # Export environment variables
          export NGC_API_KEY="${NGC_API_KEY}"
          export NVIDIA_API_KEY="${NVIDIA_API_KEY}"
          export MONGO_USERNAME="${MONGO_USERNAME:-admin}"
          export MONGO_PASSWORD="${MONGO_PASSWORD:-password}"
          export REDIS_PASSWORD="${REDIS_PASSWORD:-password}"
          
          # Source the .env file
          if [ -f ~/.env ]; then
            export $(grep -v '^#' ~/.env | xargs)
          fi
          
          cd deploy
          
          # Pull images (ignore failures for local builds)
          echo "Pulling Docker images..."
          docker compose -f docker-compose.yaml pull --ignore-pull-failures || true
          
          # Start services with MLflow enabled
          echo "Starting services..."
          docker compose -f docker-compose.yaml up -d --build
          
          echo ""
          echo "✓ Docker Compose services started"
          echo ""
          echo "Container Status:"
          docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

      # ==========================================
      # PHASE 4b: Execute Notebook (Documentation)
      # ==========================================
      # Run notebook with most cells skipped to generate HTML output for documentation.
      # Actual deployment is done in the previous step.
      - name: Execute Notebook - Generate HTML Output
        continue-on-error: true
        env:
          NGC_API_KEY: ${{ secrets.NGC_API_KEY }}
          NVIDIA_API_KEY: ${{ secrets.NVIDIA_API_KEY }}
        run: |
          echo "========================================"
          echo "  EXECUTING NOTEBOOK (Documentation Only)"
          echo "========================================"
          echo ""
          echo "Note: Most cells skipped. Deployment already done via Docker Compose."
          echo ""
          
          mkdir -p notebook_output
          
          cd notebooks
          
          # Execute notebook with almost all cells skipped
          # Only run markdown cells for documentation purposes
          # Skip all code cells: 3, 5, 7, 9, 10, 13-109
          python ../notebook_runner_nbclient.py \
            -f ai-model-distillation-financial-data.ipynb \
            --output-dir ../notebook_output \
            --timeout 600 \
            --skip-deps-check \
            --skip-cells 3 5 7 9 10 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 \
            -e NGC_API_KEY="${NGC_API_KEY}" \
            -e NVIDIA_API_KEY="${NVIDIA_API_KEY}" \
            || echo "⚠ Notebook execution had issues, but deployment succeeded"
          
          echo ""
          echo "Generated files:"
          ls -la ../notebook_output/ 2>/dev/null || echo "No output files generated"

      # ==========================================
      # PHASE 5: Wait for Services to Initialize
      # ==========================================
      - name: Wait for Services to Initialize
        run: |
          echo "========================================"
          echo "  WAITING FOR SERVICES TO INITIALIZE"
          echo "========================================"
          
          # Wait for initial startup
          sleep 45
          
          echo "Container Status:"
          docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

      # ==========================================
      # PHASE 6: Load Test Data from Huggingface
      # ==========================================
      # Since Part 1 (Teacher Model) is skipped, we load test data directly
      # from Huggingface and transform it to Data Flywheel format.
      # Dataset: ic-fspml/stock_news_sentiment
      - name: Load Test Data from Huggingface
        run: |
          echo "========================================"
          echo "  LOADING TEST DATA FROM HUGGINGFACE"
          echo "========================================"
          echo ""
          echo "Data Source: ic-fspml/stock_news_sentiment"
          echo "Purpose: Provide test data since Part 1 (Teacher Model) is skipped"
          echo ""
          
          # Install required Python packages
          pip install --break-system-packages datasets "elasticsearch>=8.0.0,<9.0.0"
          
          # Create Python script for data loading
          cat > /tmp/load_data.py << 'EOFPYTHON'
          import sys
          from datetime import datetime
          from datasets import load_dataset
          from elasticsearch import Elasticsearch
          
          WORKLOAD_ID = "news_classifier"
          CLIENT_ID = "ci-test-5k"
          ES_URL = "http://localhost:9200"
          ES_INDEX = "flywheel"
          MIN_RECORDS = 500
          
          print("Downloading dataset from Huggingface: ic-fspml/stock_news_sentiment")
          ds = load_dataset("ic-fspml/stock_news_sentiment")
          print(f"Dataset loaded. Train split has {len(ds['train'])} records")
          
          es = Elasticsearch([ES_URL])
          
          if not es.indices.exists(index=ES_INDEX):
              es.indices.create(index=ES_INDEX, body={"settings": {"number_of_shards": 1, "number_of_replicas": 0}})
              print(f"Created Elasticsearch index: {ES_INDEX}")
          else:
              print(f"Elasticsearch index {ES_INDEX} already exists")
          
          records_loaded = 0
          for i, item in enumerate(ds["train"]):
              if records_loaded >= MIN_RECORDS:
                  break
              headline = item.get("article_headline", item.get("headline", ""))
              if not headline:
                  continue
              label = item.get("article_type", "unknown")
              timestamp = int(datetime.utcnow().timestamp()) + i
              doc = {
                  "timestamp": timestamp,
                  "workload_id": WORKLOAD_ID,
                  "client_id": CLIENT_ID,
                  "request": {
                      "model": "meta/llama-3.3-70b-instruct",
                      "messages": [
                          {"role": "system", "content": "You are a financial news classifier."},
                          {"role": "user", "content": f"Classify this headline: {headline}"}
                      ]
                  },
                  "response": {
                      "choices": [{"message": {"role": "assistant", "content": f"[[[{label}]]]"}}]
                  }
              }
              es.index(index=ES_INDEX, document=doc)
              records_loaded += 1
              if records_loaded % 100 == 0:
                  print(f"  Loaded {records_loaded} records...")
          
          es.indices.flush(index=ES_INDEX)
          es.indices.refresh(index=ES_INDEX)
          print(f"\nSuccessfully loaded {records_loaded} records to Elasticsearch")
          print(f"  - workload_id: {WORKLOAD_ID}, client_id: {CLIENT_ID}, index: {ES_INDEX}")
          count = es.count(index=ES_INDEX)["count"]
          print(f"  - Total records in index: {count}")
          EOFPYTHON
          
          # Execute the Python script
          python3 /tmp/load_data.py
          
          echo ""
          echo "✓ Test data loaded successfully"

      # ==========================================
      # PHASE 7: Comprehensive Service Health Check
      # ==========================================
      - name: Comprehensive Service Health Check
        id: health_check
        run: |
          echo "========================================"
          echo "  COMPREHENSIVE SERVICE HEALTH CHECK"
          echo "========================================"
          echo ""
          
          HEALTH_STATUS="PASS"
          FAILED_SERVICES=""
          
          # ---------------------------------------------------------------
          # 1. Check all containers are running
          # ---------------------------------------------------------------
          echo "--- 1. Container Status Check ---"
          EXPECTED_SERVICES="api celery_worker celery_parent_worker redis mongodb elasticsearch mlflow"
          
          for service in $EXPECTED_SERVICES; do
            CONTAINER_STATUS=$(docker ps --filter "name=$service" --format "{{.Status}}" | head -1)
            if [ -n "$CONTAINER_STATUS" ] && echo "$CONTAINER_STATUS" | grep -q "Up"; then
              echo "✓ $service: $CONTAINER_STATUS"
            else
              echo "✗ $service: NOT RUNNING or UNHEALTHY"
              HEALTH_STATUS="FAIL"
              FAILED_SERVICES="${FAILED_SERVICES}$service "
            fi
          done
          echo ""
          
          # ---------------------------------------------------------------
          # 2. API Server Health Check
          # ---------------------------------------------------------------
          echo "--- 2. API Server Health Check ---"
          
          # Test /api/jobs endpoint (primary endpoint)
          API_JOBS=$(curl -s -o /dev/null -w "%{http_code}" "http://localhost:8000/api/jobs" 2>/dev/null || echo "000")
          if [ "$API_JOBS" = "200" ]; then
            echo "✓ API /api/jobs: HTTP $API_JOBS"
          else
            echo "✗ API /api/jobs: HTTP $API_JOBS (CRITICAL)"
            HEALTH_STATUS="FAIL"
          fi
          
          # Test /docs endpoint (FastAPI Swagger UI)
          API_DOCS=$(curl -s -o /dev/null -w "%{http_code}" "http://localhost:8000/docs" 2>/dev/null || echo "000")
          if [ "$API_DOCS" = "200" ]; then
            echo "✓ API Documentation: HTTP $API_DOCS"
          else
            echo "✗ API Documentation: HTTP $API_DOCS"
            HEALTH_STATUS="FAIL"
          fi
          
          # Test /openapi.json endpoint
          API_OPENAPI=$(curl -s -o /dev/null -w "%{http_code}" "http://localhost:8000/openapi.json" 2>/dev/null || echo "000")
          if [ "$API_OPENAPI" = "200" ]; then
            echo "✓ OpenAPI Schema: HTTP $API_OPENAPI"
          else
            echo "✗ OpenAPI Schema: HTTP $API_OPENAPI"
            HEALTH_STATUS="FAIL"
          fi
          echo ""
          
          # ---------------------------------------------------------------
          # 3. Elasticsearch Health Check
          # ---------------------------------------------------------------
          echo "--- 3. Elasticsearch Health Check ---"
          
          ES_HEALTH=$(curl -s "http://localhost:9200/_cluster/health" 2>/dev/null)
          ES_STATUS=$(echo "$ES_HEALTH" | grep -o '"status":"[^"]*"' | cut -d'"' -f4)
          ES_NODES=$(echo "$ES_HEALTH" | grep -o '"number_of_nodes":[0-9]*' | cut -d':' -f2)
          
          if [ "$ES_STATUS" = "green" ] || [ "$ES_STATUS" = "yellow" ]; then
            echo "✓ Elasticsearch cluster status: $ES_STATUS"
            echo "  - Number of nodes: $ES_NODES"
          else
            echo "✗ Elasticsearch cluster status: $ES_STATUS"
            HEALTH_STATUS="FAIL"
          fi
          
          # Check flywheel index
          ES_INDEX_CHECK=$(curl -s -o /dev/null -w "%{http_code}" "http://localhost:9200/flywheel" 2>/dev/null || echo "000")
          if [ "$ES_INDEX_CHECK" = "200" ]; then
            echo "✓ Flywheel index: exists"
            DOC_COUNT=$(curl -s "http://localhost:9200/flywheel/_count" 2>/dev/null | grep -o '"count":[0-9]*' | cut -d':' -f2 || echo "0")
            echo "  - Document count: $DOC_COUNT"
          else
            echo "✗ Flywheel index: not found (HTTP $ES_INDEX_CHECK)"
            HEALTH_STATUS="FAIL"
          fi
          echo ""
          
          # ---------------------------------------------------------------
          # 4. Redis Health Check
          # ---------------------------------------------------------------
          echo "--- 4. Redis Health Check ---"
          
          REDIS_CONTAINER=$(docker ps -qf "name=redis" | head -1)
          if [ -n "$REDIS_CONTAINER" ]; then
            REDIS_PING=$(docker exec $REDIS_CONTAINER redis-cli -a "${REDIS_PASSWORD:-password}" ping 2>/dev/null || echo "FAILED")
            if [ "$REDIS_PING" = "PONG" ]; then
              echo "✓ Redis PING: $REDIS_PING"
            else
              echo "✗ Redis PING failed: $REDIS_PING"
              HEALTH_STATUS="FAIL"
            fi
          else
            echo "✗ Redis container not found"
            HEALTH_STATUS="FAIL"
          fi
          echo ""
          
          # ---------------------------------------------------------------
          # 5. MongoDB Health Check
          # ---------------------------------------------------------------
          echo "--- 5. MongoDB Health Check ---"
          
          MONGO_CONTAINER=$(docker ps -qf "name=mongodb" | head -1)
          if [ -n "$MONGO_CONTAINER" ]; then
            MONGO_PING=$(docker exec $MONGO_CONTAINER mongosh --eval "db.runCommand('ping').ok" --quiet -u "${MONGO_USERNAME:-admin}" -p "${MONGO_PASSWORD:-password}" 2>/dev/null || echo "0")
            if [ "$MONGO_PING" = "1" ]; then
              echo "✓ MongoDB PING: OK"
            else
              echo "✗ MongoDB PING failed"
              HEALTH_STATUS="FAIL"
            fi
          else
            echo "✗ MongoDB container not found"
            HEALTH_STATUS="FAIL"
          fi
          echo ""
          
          # ---------------------------------------------------------------
          # 6. Celery Workers Health Check
          # ---------------------------------------------------------------
          echo "--- 6. Celery Workers Health Check ---"
          
          CELERY_WORKER=$(docker ps -qf "name=celery_worker" 2>/dev/null | head -1)
          if [ -n "$CELERY_WORKER" ]; then
            CONTAINER_STATUS=$(docker inspect --format='{{.State.Status}}' "$CELERY_WORKER" 2>/dev/null || echo "unknown")
            if [ "$CONTAINER_STATUS" = "running" ]; then
              echo "✓ Celery Worker: Running"
            else
              echo "✗ Celery Worker: Status=$CONTAINER_STATUS"
              HEALTH_STATUS="FAIL"
            fi
          else
            echo "✗ Celery Worker container not found"
            HEALTH_STATUS="FAIL"
          fi
          
          CELERY_PARENT=$(docker ps -qf "name=celery_parent_worker" 2>/dev/null | head -1)
          if [ -n "$CELERY_PARENT" ]; then
            CONTAINER_STATUS=$(docker inspect --format='{{.State.Status}}' "$CELERY_PARENT" 2>/dev/null || echo "unknown")
            if [ "$CONTAINER_STATUS" = "running" ]; then
              echo "✓ Celery Parent Worker: Running"
            else
              echo "✗ Celery Parent Worker: Status=$CONTAINER_STATUS"
              HEALTH_STATUS="FAIL"
            fi
          else
            echo "✗ Celery Parent Worker container not found"
            HEALTH_STATUS="FAIL"
          fi
          echo ""
          
          # ---------------------------------------------------------------
          # 7. MLflow Health Check
          # ---------------------------------------------------------------
          echo "--- 7. MLflow Health Check ---"
          
          MLFLOW_CONTAINER=$(docker ps -qf "name=mlflow" 2>/dev/null | head -1)
          if [ -n "$MLFLOW_CONTAINER" ]; then
            CONTAINER_STATUS=$(docker inspect --format='{{.State.Status}}' "$MLFLOW_CONTAINER" 2>/dev/null || echo "unknown")
            if [ "$CONTAINER_STATUS" = "running" ]; then
              echo "✓ MLflow Container: Running"
              
              # Check MLflow HTTP endpoint
              MLFLOW_HTTP=$(curl -s -o /dev/null -w "%{http_code}" "http://localhost:5000/" 2>/dev/null || echo "000")
              if [ "$MLFLOW_HTTP" = "200" ]; then
                echo "✓ MLflow UI: HTTP $MLFLOW_HTTP"
              else
                echo "✗ MLflow UI: HTTP $MLFLOW_HTTP"
                HEALTH_STATUS="FAIL"
              fi
            else
              echo "✗ MLflow Container: Status=$CONTAINER_STATUS"
              HEALTH_STATUS="FAIL"
            fi
          else
            echo "✗ MLflow container not found"
            HEALTH_STATUS="FAIL"
          fi
          echo ""
          
          # ---------------------------------------------------------------
          # Final Summary
          # ---------------------------------------------------------------
          echo "========================================"
          if [ "$HEALTH_STATUS" = "PASS" ]; then
            echo "✓ ALL HEALTH CHECKS PASSED"
            echo "========================================"
            echo ""
            echo "All services are running and healthy."
            echo "The Data Flywheel deployment is ready for use."
          else
            echo "✗ HEALTH CHECK FAILED"
            echo "========================================"
            echo ""
            echo "Failed services: $FAILED_SERVICES"
            echo ""
            echo "Container Status:"
            docker ps -a --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
            exit 1
          fi
          
          echo ""
          echo "health_status=$HEALTH_STATUS" >> $GITHUB_OUTPUT

      # ==========================================
      # PHASE 8: Verify API Functionality
      # ==========================================
      - name: Verify API Functionality
        run: |
          echo "========================================"
          echo "  VERIFYING API FUNCTIONALITY"
          echo "========================================"
          echo ""
          
          # Test GET /api/jobs
          echo "--- Testing GET /api/jobs ---"
          RESPONSE=$(curl -s "http://localhost:8000/api/jobs")
          echo "Response: $RESPONSE"
          echo ""
          
          # Test API docs access
          echo "--- Testing API Documentation ---"
          DOC_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "http://localhost:8000/docs")
          echo "API Docs Status: HTTP $DOC_STATUS"
          echo ""
          
          # Display service endpoints
          echo "========================================"
          echo "  SERVICE ENDPOINTS"
          echo "========================================"
          echo ""
          echo "  - API Server:        http://localhost:8000"
          echo "  - API Documentation: http://localhost:8000/docs"
          echo "  - OpenAPI Schema:    http://localhost:8000/openapi.json"
          echo "  - Elasticsearch:     http://localhost:9200"
          echo "  - MongoDB:           localhost:27017"
          echo "  - Redis:             localhost:6379"
          echo "  - MLflow UI:         http://localhost:5000"
          echo ""

      - name: Upload Notebook HTML
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: notebook-html-output
          path: notebook_output/*.html
          retention-days: 30
          if-no-files-found: warn

      - name: Collect Logs on Failure
        if: failure()
        run: |
          echo "========================================"
          echo "  COLLECTING LOGS FOR DEBUGGING"
          echo "========================================"
          
          echo ""
          echo "--- Container Status ---"
          docker ps -a --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
          
          echo ""
          echo "--- API Container Logs ---"
          docker compose -f deploy/docker-compose.yaml logs api --tail=100 2>/dev/null || echo "No API logs available"
          
          echo ""
          echo "--- Celery Worker Logs ---"
          docker compose -f deploy/docker-compose.yaml logs celery_worker --tail=50 2>/dev/null || echo "No Celery logs available"

      # ==========================================
      # Email Notification
      # ==========================================
      - name: Set Result Output
        id: set_result
        if: always()
        run: |
          if [ "${{ needs.preflight.result }}" == "success" ] && \
             [ "${{ job.status }}" == "success" ]; then
            echo "RESULT=PASS" >> $GITHUB_OUTPUT
          else
            echo "RESULT=FAIL" >> $GITHUB_OUTPUT
          fi

      - name: Send Email Notification
        uses: dawidd6/action-send-mail@6e71c855c9a091d80a519621b9fd3e8d252ca40c
        if: always() && env.ENABLE_EMAIL_NOTIFICATION == 'true'
        with:
          server_address: smtp.gmail.com
          server_port: 587
          username: ${{ secrets.SMTP_USERNAME }}
          password: ${{ secrets.SMTP_PASSWORD }}
          subject: "CI Result: AI Model Distillation for Financial Data - ${{ steps.set_result.outputs.RESULT }}"
          to: Github-Action-Blueprint-QA@nvidia.com
          from: github-workflow-notification@gmail.com
          html_body: |
            <h2>AI Model Distillation for Financial Data CI Notification</h2>
            
            <p><strong>Repository:</strong> ${{ github.repository }}</p>
            <p><strong>Branch:</strong> ${{ github.ref_name }}</p>
            <p><strong>Commit:</strong> ${{ github.sha }}</p>
            <p><strong>Result:</strong> <span style="color: ${{ steps.set_result.outputs.RESULT == 'PASS' && 'green' || 'red' }}; font-weight: bold;">${{ steps.set_result.outputs.RESULT }}</span></p>
            
            <h3>Job Results</h3>
            <table border="1" cellpadding="5" cellspacing="0">
              <tr><th>Job</th><th>Status</th></tr>
              <tr><td>Preflight</td><td>${{ needs.preflight.result }}</td></tr>
              <tr><td>Deploy & Test</td><td>${{ job.status }}</td></tr>
            </table>
            
            <h3>Test Configuration</h3>
            <ul>
              <li><strong>Notebook:</strong> ai-model-distillation-financial-data.ipynb</li>
              <li><strong>Data Source:</strong> Huggingface (ic-fspml/stock_news_sentiment)</li>
              <li><strong>NMP Status:</strong> Skipped (requires Kubernetes)</li>
            </ul>
            
            <p><a href="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}">View Workflow Run</a></p>

  # ============================================
  # SUMMARY: Generate final report
  # ============================================
  summary:
    name: Generate Summary
    needs: [preflight, deploy-and-test]
    if: always()
    runs-on: ubuntu-latest
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts
        continue-on-error: true

      - name: Generate Summary Report
        run: |
          echo "# AI Model Distillation for Financial Data CI Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Generated at: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Job Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Preflight | ${{ needs.preflight.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Deploy & Test | ${{ needs.deploy-and-test.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Test Configuration" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Notebook**: \`ai-model-distillation-financial-data.ipynb\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Data Source**: Huggingface (\`ic-fspml/stock_news_sentiment\`)" >> $GITHUB_STEP_SUMMARY
          echo "- **Deployment**: Docker Compose only (NMP skipped)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## NMP Limitation Note" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "> **Important**: The full notebook requires NMP (NeMo Microservices Platform) which needs Kubernetes/Minikube." >> $GITHUB_STEP_SUMMARY
          echo "> CI runners only have Docker, so NMP-related cells (Part 1: Teacher Model, Part 2: Job execution) are skipped." >> $GITHUB_STEP_SUMMARY
          echo "> This CI validates Docker Compose deployment and service health only." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Validated Services" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Service | Port | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|---------|------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| API Server | 8000 | ✓ |" >> $GITHUB_STEP_SUMMARY
          echo "| Elasticsearch | 9200 | ✓ |" >> $GITHUB_STEP_SUMMARY
          echo "| MongoDB | 27017 | ✓ |" >> $GITHUB_STEP_SUMMARY
          echo "| Redis | 6379 | ✓ |" >> $GITHUB_STEP_SUMMARY
          echo "| Celery Workers | - | ✓ |" >> $GITHUB_STEP_SUMMARY
          echo "| MLflow | 5000 | ✓ |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Notebook HTML output is available in the Artifacts section." >> $GITHUB_STEP_SUMMARY
